# Parameters of the Seq2Seq Model

# Device-sensitive parameters
amr_data_dir: &amr_data_dir data/AMR/
numberbatch: &numberbatch data/numberbatch/numberbatch-19.08.en_ms.txt


# Model parameters
model:
  model_type: STOG
  use_must_copy_embedding: False
  use_char_cnn: True
  use_pos_decoder: False
  use_coverage: False
  use_aux_encoder: False
  use_bert: True
  universal_postags: True
  multilingual: True

  bert:
    pretrained_model_dir: data/bert-base-multilingual-cased
    hidden_size: 768

  encoder_token_embedding:
    num_embeddings:
    vocab_namespace: 'encoder_token_ids'
    embedding_dim: 300
    padding_index: 0
    dropout: 0.33
    pretrained_file: *numberbatch
    trainable: True

  encoder_pos_embedding:
    num_embeddings:
    vocab_namespace: 'pos_tags'
    embedding_dim: 100
    padding_index: 0
    dropout: 0

  encoder_must_copy_embedding:
    num_embeddings: 3
    vocab_namespace: 'must_copy_tags'
    embedding_dim: 50
    padding_index:
    dropout: 0

  encoder_char_embedding:
    num_embeddings:
    vocab_namespace: 'encoder_token_characters'
    embedding_dim: &encoder_char_embedding_dim 100
    padding_index: 0
    dropout: 0
    pretrained_file:

  encoder_char_cnn:
    embedding_dim: *encoder_char_embedding_dim
    num_filters: 100
    ngram_filter_sizes: [3]

  decoder_token_embedding:
    num_embeddings:
    vocab_namespace: 'decoder_token_ids'
    embedding_dim: 300
    padding_index: 0
    dropout: 0.33
    pretrained_file: *numberbatch
    data_type: AMR

  decoder_pos_embedding:
    num_embeddings:
    vocab_namespace: 'pos_tags'
    embedding_dim: 100
    padding_index: 0
    dropout: 0

  decoder_coref_embedding:
    num_embeddings: 500
    embedding_dim: 50
    padding_index: 0
    dropout: 0.33

  decoder_char_embedding:
    num_embeddings:
    vocab_namespace: 'decoder_token_characters'
    embedding_dim: &decoder_char_embedding_dim 100
    padding_index: 0
    dropout: 0.33
    pretrained_file:

  decoder_char_cnn:
    embedding_dim: *decoder_char_embedding_dim
    num_filters: 100
    ngram_filter_sizes: [3]

  encoder:
    input_size:
    hidden_size: 512
    num_layers: 2
    use_highway: False
    dropout: 0.33

  decoder:
    input_size:
    hidden_size: &decoder_hidden_size 1024
    num_layers: 2
    use_highway: False
    dropout: 0.33

  source_attention:
    attention_function: mlp
    coverage: True

  coref_attention:
    attention_function: mlp
    share_linear: True

  generator:
    input_size: *decoder_hidden_size
    # Specify vocab_size and pad_idx dynamically
    vocab_size:
    pad_idx:
    force_copy: False
    source_copy: &generator_source_copy False
    mult_token_mapping: &generator_mult_token_mapping data/numberbatch/{}_en_neighbors_model.json
    generator_type: pg


  graph_decoder:
    decode_algorithm: 'greedy'
    input_size: *decoder_hidden_size
    edge_node_hidden_size: 256
    edge_label_hidden_size: 128
    dropout: 0.33

  mimick_test:

    data:
      - !!python/list [ms, data/AMR/amr_2.0/test_ms.txt.features.recat]
    prediction: test.mult.ms.pred.txt
    batch_size: 32
    eval_script: scripts/eval_test.sh
    smatch_dir: tools/amr-evaluation-tool-enhanced
    word_splitter: &word_splitter data/bert-base-multilingual-cased/bert-base-multilingual-cased-vocab.txt
    extra_check: &lang_check True


# Vocabulary
vocab:
#  fixed_vocab: True
  non_padded_namespaces: [must_copy_tags, coref_tags]
  min_count:
      encoder_token_ids: 1
      decoder_token_ids: 1
  max_vocab_size:
      encoder_token_ids: #18000
      decoder_token_ids: #12200


# Data parameters

data:
  data_dir: *amr_data_dir
  train_data:
#    - !!python/list [en, amr_2.0/train.txt.features.recat]
#    - !!python/list [de, panl_en_ms/train_de.txt.features.recat]
#    - !!python/list [es, panl_en_ms/train_es.txt.features.recat]
    - !!python/list [ms, panl_en_ms/train_ms.txt.features.recat]
  dev_data:
#    - !!python/list [en, amr_2.0/dev.txt.features.recat]
#    - !!python/list [de, panl_en_ms/dev_de.txt.features.recat]
#    - !!python/list [es, panl_en_ms/dev_es.txt.features.recat]
    - !!python/list [ms, panl_en_ms/dev_ms.txt.features.recat]
  test_data:
    - !!python/list [ms, panl_en_ms/dev_ms.txt.features.recat]

  data_type: AMR
  batch_first: True
  iterator:
    train_batch_size: &train_batch_size 32
    test_batch_size: 32
    iter_type: BucketIterator
    sorting_keys:  # (field, padding type)
      - [tgt_tokens, num_tokens]
  pretrain_token_emb: *numberbatch
  pretrain_char_emb:
  word_splitter: *word_splitter
  source_copy: *generator_source_copy
  mult_token_mapping: *generator_mult_token_mapping
  extra_check: *lang_check


# Training parameters
environment:
  recover: False
  seed: 1
  numpy_seed: 1
  torch_seed: 1
  serialization_dir: &serialization_dir models/xl-amr_lang_specific_ms_par
  file_friendly_logging: False
  gpu: True
  cuda_device: 3

trainer:
  device:
  no_grad: []
  optimizer_type: adam
  learning_rate: 0.001
  max_grad_norm: 5.0
  batch_size: *train_batch_size
  shuffle: True
  epochs: 120
  dev_metric: +F1
  serialization_dir: *serialization_dir
  model_save_interval:

test:
  evaluate_on_test: False
